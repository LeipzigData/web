\documentclass[a4paper,11pt]{article}
\usepackage{a4wide,amsmath,ngerman,url,graphicx}
\usepackage[utf8]{inputenc}
\parskip4pt
\parindent0pt

\newcommand{\br}[1]{\left(#1\right)}
\newcommand{\erf}{\mathrm{erf}}
\newcommand{\m}{\cdot}

\title{Analyse der Coronastatistiken} 
\author{Hans-Gert Gräbe, Leipzig}
\date{Version vom 13. April 2020}

\begin{document}
\maketitle

Dieser Text bezieht sich auf die im Verzeichnis
\url{http://leipzig-data.de/demo/Corona-20/Code} verfügbaren Materialien.

\section{Datenbasis}

Als Datenbasis werden die von der Johns Hopkins Universität (JHU) als
Excel-Datei veröffentlichten Daten\footnote{Siehe deren github-Projekt
  \url{https://github.com/CSSEGISandData/COVID-19}.} zur Entwicklung der
weltweit registrierten Covid-19-Fälle verwendet. Die Statistik listet pro Land
und Tag die kumulierte Zahl der Infizierten, der Genesenen und der Todesfälle
auf.

Die Statistiken sind natürlich zu hinterfragen, da sich jedem Physiker die
Haare sträuben würden, wenn die Debatte auf die messmethodische Grundlage
dieser Datenerhebungen, also die Einheitlichkeit des Modells, zu sprechen
kommt.  Dennoch gibt es wohl derzeit nichts Besseres. Auch die Rolle der
privaten (!)  JHU und deren enge Verflechtung mit dem „Datenadel“ aus dem
Silicon Valley kann hinterfragt werden, siehe dazu (Rügemer 2020).  Spannend
auf der anderen Seite, dass es genau dieser „Datenadel“ ist, der solche Daten
zusammenträgt und der Allgemeinheit -- im Gegensatz etwa zu der im deutschen
Sprachraum viel zitierten \url{https://statista.com} -- in einem
maschinenlesbaren Format ohne Bezahlschranken zur Verfügung stellt.

Eine solche kritische Würdigung muss diesem Text vorangestellt werden, um die
folgenden Rechnungen ins rechte Licht zu rücken, denn sie sind nicht als
Beitrag zur Corona-Debatte selbst gedacht, wie dies in den ersten Tagen
mehrfach falsch verstanden wurde, sondern soll die Möglichkeiten aufzeigen,
die fortgeschrittene Oberstufenschülerinnen und -schüler heute zu einer
eigenen Analyse von Daten haben.  Der komplexe Gegenstand „CAS in der Schule“
ist in meinem Buch (Gräbe 2018) genauer dargestellt. 

\section{Installation} 

Zunächst muss das git Repo der JHU lokal geklont und der Pfad im Skript
\texttt{extractData.pl} eingetragen werden.  Die Daten werden für ausgewählte
Länder mit diesem Perl-Skript für die weitere Verarbeitung aufbereitet und in
einer Datei \texttt{BasicData.txt} gespeichert, um dann mit dem freien CAS
\emph{Maxima} weiterverarbeitet zu werden.

\section{Datentransformation}

In der Datei \texttt{BasicData.txt} sind die Daten für jedes der ausgewählten
Länder in einem Array mit drei Einträgen (infected, recovered, dead)
gespeichert, die im Maxima-Skript \texttt{skript.m}\footnote{Dies ist eine
  reine Textdatei mit Code-Schnipseln, die nicht für den Batchbetrieb
  konzipiert ist.} in einer Funktion \texttt{getland(Land)} zunächst einmal zu
Paaren $(t,y_t)$ ergänzt werden, wobei $t$ für den Tag des Jahres 2020
($1=01.01.2020$, \textbf{Tag 100 ist also Donnerstag der 9. April}) und $y_t$
für die Zahl der Fälle aus dem jeweiligen Record stehen.

\section{Fitting}

Alle Grafiken zu Prognosen der Daten, die ich bisher gesehen habe, gehen von
einer „Glockenkurve“ aus.  Das kann natürlich nur die Entwicklung der Zuwächse
pro Tag abbilden, die aus den kumulierten Daten zunächst als $d_t=y_t-y_{t-1}$
extrahiert werden müssen. Dies geschieht mit der im Skript definierten
Maxima-Funktion \texttt{Delta}\footnote{\texttt{reverse(rest(reverse(l)))}
  entfernt das letzte Element der Liste, \texttt{append([0],...)} fügt vorn
  eine $0$ an, womit eine Liste $l_1$ entsteht, in der alle Einträge um eine
  Position nach rechts verschoben sind. $l-l_1$ berechnet die Differenz der
  Listen, was in Maxima (und anderen CAS) als Subtraktion der entsprechenden
  Vektoren implementiert ist.}.

Zur Abschätzung des Verlaufs längs einer Glockenkurve wird üblicherweise die
Statistikfunktion $C\m\exp\br{-\br{\frac{t-m}{s}}^2}$ verwendet, die ich als
Kurvenschar $f(t)=\exp\br{c-\br{\frac{t-m}{s}}^2}$ zur Parameterschätzung auf
die Daten $(t,d_t)$ ansetze.  Die (kumulierten) Originaldaten $(t,y_t)$
sollten dann auf die Funktion
\begin{align*}
  h(x)&=\int_0^x{\exp\br{c-\br{\frac{t-m}{s}}^2}}\,dt\\ &=\exp(c)\m s\m
  \int_{-\frac{m}{s}}^{\frac{x-m}{s}}{\exp\br{-u^2}}\,du\\ &=\frac12 \sqrt{\pi}\m
  \exp(c)\m s\m \br{\erf\br{\frac{x-m}{s}}+ \erf\br{\frac{m}{s}}}
\end{align*}
matchen, wobei $\erf(x)$ für die Fehlerfunktion steht und im zweiten Schritt
die Variablensubstitution $u=\frac{x-m}{s}$ mit $du=s\m dt$ erfolgte.

Nun sind die Parameter $(c,s,m)$ dieser Kurvenschar so zu fitten, dass die
ermittelte Kurve besonders gut auf die Daten passt.  In \emph{Maxima} kann
dazu das Paket \emph{lsquares} verwendet werden.  

Wir hätten natürlich auch versuchen können, die Originaldaten auf die Schar
$h(t)$ zu fitten, aber Fitting auf nicht polynomialen Kurvenscharen ist eine
schwierige und numerisch wenig stabile Angelegenheit, bei der Maxima schnell
an seine Grenzen kommt (und die Ergebnisse anderer CAS sehr genau zu
analysieren sind, da die Fitting-Ergebnisse stark von Startwerten der dabei
eingesetzten Verfahren abhängen).

\emph{Maxima} kommt auch beim Fitting der Schar $f(t)$ zu keinem Ergebnis.
Einen einfacheren, nämlich quadratischen Zusammenhang
$g(t)=c-\br{\frac{t-m}{s}}^2$ erhält man, wenn man zu Paaren $(t,\log(d_t))$
übergeht.  Damit lassen sich dann die Fittingparameter weitgehend stabil
berechnen. Dafür müssen aber vorher Datenpunkte aussortiert werden, wo $d_t=0$
ist.

Generell kann es sinnvoll sein, für ein gutes Fitting Datenpunkte unterhalb
einer Schwelle auszusortieren. Eine solche Schwelle $S$ ist als weiterer
Parameter im Skript in der Funktion \texttt{FittingDelta(G,S)} vorgesehen. Für
die meisten Datensätze ist die Schwelle 50 eine gute Wahl. $G$ ist die Liste
$(t,y_t)$ der zu fittenden Datenpunkte.

Details sind im Skript \texttt{skript.m} zu finden.

\section{Ergebnisse}

Die Rechnungen werden für jedes der Länder nun wie folgt ausgeführt:
\begin{enumerate}
\item Fasse mit \texttt{l:getData(Land)} die drei Datensätze für das Land als
  Tripel von Listen $(t,y_t)$ zusammen.
\item Berechne für jeden der drei Datensätze mit
  \texttt{getFittingFunctions(l,S)} das Fitting auf $(t,\log(d_t))$ gegen die
  Funktion $g(t)$ und verwende die berechneten drei Fittings, um Funktionen
  $h_1(t)$ (für infected), $h_2(t)$ (für recovered) und $h_3(t)$ (für dead) zu
  schätzen.
\item Erzeuge daraus einen Plot, welcher die Datenpunkte und die drei Kurven
  in verschiedenen Farben (rot für infected, grün für recovered, blau für
  dead) ausgibt.  
\end{enumerate}
Bei erfolgreichem Fitting ist eine gute Übereinstimmung der jeweiligen
Kurve\footnote{Für $x=\frac{m}{s}\approx 10$ kann $\erf(x)=1$ gesetzt werden.
Ist dieser Wert im Fitting deutlich anders, ist das Fitting unbrauchbar.}
\begin{gather*}
  h(t)=A\br{\erf\br{B(t-m)}+1}
\end{gather*}
mit den Datenpunkten zu verzeichnen.  Die berechneten Parameter haben folgende
Bedeutung:
\begin{itemize}
\item $m$ -- Tag, an dem die Spitze in den Inkrementdaten erreicht ist.
\item $B=\frac{1}{s}$ -- $\sigma=\frac{s}{\sqrt{2}}$ ist die
  Standardabweichung.
\item $2A$ -- Zahl der am Ende insgesamt betroffenen Personen. 
\end{itemize}

Auf der Basis der Daten vom 13.04.2020 lassen sich folgende Szenarien (!) für
die Länder Deutschland, Italien, Spanien, Österreich und Schweden erstellen,
siehe auch die Abbildungen.

Alles begann in der chinesischen Provinz Hubei (mit 58.5 Mio. Einwohnern hat
sie eine mit Italien vergleichbare Einwohnerzahl).  Auch die dortige
Entwicklung ist dargestellt\footnote{Maxima hängt sich bei der Berechnung
  dieses Fittings allerdings schnell auf.}.

\begin{center}
  \begin{tabular}{|l|r|r|r|}\hline
    & \multicolumn{1}{|c|}{$m$} & \multicolumn{1}{|c|}{$s$}
    & \multicolumn{1}{|c|}{$A$} \\\hline
    \multicolumn{4}{|c|}{\bf Deutschland}\\\hline
    infected   & 91.94 & 13.51 &  68896\\
    recovered  & 97.01 &  8.58 &  32967\\
    dead       &103.11 & 14.80 &   3241\\\hline
    \multicolumn{4}{|c|}{\bf Italien}\\\hline
    infected   & 88.34 & 16.30 &  87225\\
    recovered  &101.35 & 21.97 &  31380\\
    dead       & 90.31 & 16.07 &  11520\\\hline
    \multicolumn{4}{|c|}{\bf Spanien}\\\hline
    infected   & 91.11 & 12.32 &  90706\\
    recovered  &101.77 & 15.78 &  54984\\
    dead       & 93.33 & 11.88 &   9611\\\hline
    \multicolumn{4}{|c|}{\bf Österreich}\\\hline
    infected   & 88.55 & 12.09 &   6651\\
    recovered  &100.02 & 10.68 &   5319\\
    dead       & 98.62 & 17.12 &    294\\\hline
    \multicolumn{4}{|c|}{\bf Schweden}\\\hline
    infected   &101.08 & 20.75 &   8870\\
    recovered  &112.73 & 20.12 &   4402\\
    dead       & 95.36 &  9.09 &    459\\\hline
    \multicolumn{4}{|c|}{\bf China, Provinz Hubei}\\\hline
    infected   & 41.45 & 13.12 &  26139\\
    recovered  & \multicolumn{3}{|c|}{bad fitting}\\
    dead       & 46.74 & 15.72 &   1759\\\hline
  \end{tabular}
\end{center}
Natürlich müsste am Ende die Zahl der Infizierten mit der Summe der Zahlen der
Genesenen und der Verstorbenen übereinstimmen.  Die Schätzungen sind von einer
solchen Invarianzforderung weit entfernt, was ein Licht auf die prognostische
Qualität der Schätzungen für die weitere Zukunft (drei bis vier Wochen, also
30 Tage) wirft.  Weitere Ergebnisse zeigen, dass selbst einem ohne
Fehlermeldung zurückgegebenen Fitting nicht zu trauen ist und die Güte des
Fittings genauer analysiert werden muss.  Die Ergebnisse für die Provinz Hubei
legen nahe, dass das verwendete Datenmodell für die Schätzung generell wenig
geeignet ist, da die Zahl der Infektionen selbst im Nachgang zu klein
geschätzt wird und für die Zahl der Genesenen kein brauchbares Fitting
gefunden wird, obwohl sich die Zahlen in Bereichen bewegen, wie sie auch für
europäische Länder charakteristisch sind.

\section{Logistische Funktion}

Generell ist ein Modell auf der Basis einer \emph{Logistischen Funktion}
\begin{gather*}
  u(t)=\frac{K}{1+C\m\exp(-rt)}\tag{L.1}
\end{gather*}
die anerkanntere Form der Modellierung der Ausbreitung einer Infektion, siehe
dazu den entsprechenden Wikipedia-Eintrag. 
\begin{center}
  \includegraphics[width=.8\textwidth]{LC.png}\\[1em]
  \begin{quote}
    {Logistische Kurve $u(t)=\frac{1}{1+12\exp(-2t)}$ (blau) sowie deren erste
      (rot) und zweite Ableitung (grün) Ableitung}
  \end{quote}
\end{center}
$K$ steht dabei für die Sättigungsgrenze $\lim_{t\to\infty}{u(t)}$ und $C$ ist
üblicherweise als $C=\frac{K}{u(0)}-1$ angeschrieben, was sich unmittelbar aus
der Umstellung der Formel für $u(0)$ nach $C$ ergibt. Der Wendepunkt dieser
Funktion und damit das Maximum der ersten Ableitung liegt als Nullstelle der
zweiten Ableitung bei $t_0=\frac{\log(C)}{r}$, was sich unmittelbar mit Maxima
berechnen lässt.  Dies ist gerade der Median der Kurve, sollte also mit dem
oben berechneten $m$ zusammenfallen.  

Derartige Funktionen lassen sich aber deutlich schlechter schätzen als
Funktionen, die sich wie oben auf einfache Weise auf einen polynomialen
Zusammenhang reduzieren lassen, da sie inhärent transzendent sind.  Siehe
hierzu aber die Arbeit von (Engel 2010) und die Modellierung mit
\textsc{GeoGebra} in (Elschenbroich 2020).

In (Engel 2010) wird insbesondere darauf hingewiesen, dass sich mit einer
guten Schätzung von $K$ die anderen beiden Parameter mit einem linearen
Fitting bestimmen lassen. Wir setzen dazu $C=\exp(c)$, womit sich Formel
$(L.1)$ zu
\begin{gather*}
  \log\br{\frac{K}{u(t)}-1}=c-rt  \tag{L.2}
\end{gather*}
umstellen lässt.  Der Parameter $K$ ist dabei manuell zu schätzen, so dass die
gefittete Kurve möglichst gut auf die Daten passt.

Im Skript ist das Ganze in einer Funktion \texttt{lFit(G,K0)} implementiert,
der eine Liste $G$ zu übergeben ist, in der vorab alle Datenpunkte mit $y_t\le
10$ ausgefiltert werden.  Es ergeben sich folgende Schätzungen für die Zahl
der (in der Statistik erfassten) infizierten Personen (Stand 10.04.2020):
\begin{center}
  \begin{tabular}{|l|c|c|c|c|}\hline
    Land & $K$ & $c$ & $r$ & $c/r$ \\\hline
    Deutschland & 125000 & 18.058 & 0.198 & 91.11\\
    Italien & 145000 & 17.445 & 0.207 & 84.47\\
    Österreich & 13000 & 23.053 & 0.268 & 85.90 \\
    Spanien & 160000 & 23.815 & 0269 & 88.42 \\
    China (Hebei) & 70000 & 4.664 & 0.103 & 45.12\\\hline
  \end{tabular}
\end{center}

\section{Die „Verdoppelungsdebatte“}

Anfang April 2020 kommt eine Diskussion hoch, dass man die rigiden
Beschränkungen erst aufheben könne, wenn „die Verdopplungszeit der Infektionen
größer als 14 Tage“ sei.

So meldet zum Beispiel der Deutschlandfunk am 04.04.2020\footnote{\raggedright
  \url{https://www.deutschlandfunk.de/covid19-verdopplungszeit-der-coronavirus-infektionen-in.1939.de.html?drn:news_id=1117169}}
\begin{quote}
  Die Verdopplungszeit der Ausbreitung von Coronavirus-Infektionen in
  Deutsch"|land hat sich in den vergangenen Tagen verlangsamt.

  Für ganz Deutschland liegt sie nun bei 11{,}2 Tagen. Die Lage in den
  Bundesländern ist unterschiedlich. In den großen Flächenländern liegt die
  Verdopplungszeit in Nordrhein-Westfalen bei 13{,}1 Tagen, in
  Baden-Württemberg bei 12{,}5 Tagen und in Bayern bei 9{,}7 Tagen. In Berlin
  sind es inzwischen 12{,}8 Tage, in Hamburg 12{,}4. Im Saarland hingegen
  liegt die Verdoppelungszeit bei 5{,}5 Tagen, in Sachsen bei 11{,}0 Tagen.
\end{quote}
Auch wenn dies nicht immer deutlich wird, bezieht sich die Verdopplungszeit
$v_t$ auf die kumulierten Daten und steigt deshalb bereits durch die schiere
Masse der Infizierten.  Ist $y(t)=mt+n$ ein linearer Zusammenhang, so ergibt
sich $v_t=\frac{y(t)}{m}$. Für einen annähernd linearen Zusammenhang kann man
also $v_t=\frac{y(t)}{y'(t)}$ als Schätzung nehmen.  Die Zahl lässt sich auch
aus unseren Daten leicht berechnen: Ist $y_t$ die kumulierte Zahl der
Infizierten am Tag $t$ und $d_t$ die Zahl der Neuinfektionen, so ist nach
$v_t=\frac{y_t}{d_t}$ Tagen eine Verdopplung der Infizierten erreicht, die
Zuwachsrate $d_t$ über diesen Zeitraum als konstant vorausgesetzt.  Beide
Datenreihen (Stand 10.04.2020) hatten wir schon oben extrahiert, so dass wir
eine einfache Funktion \texttt{doublePlot(Land)} schreiben können, um die
folgenden Plots zu erzeugen:
\begin{center}  
  \begin{minipage}{.33\textwidth}\centering
    \includegraphics[width=\textwidth]{Italy-DP.png}\\[1em] {Italien}
  \end{minipage}\hfill
  \begin{minipage}{.33\textwidth}\centering
    \includegraphics[width=\textwidth]{Germany-DP.png}\\[1em] {Deutschland}
  \end{minipage}\hfill
  \begin{minipage}{.33\textwidth}\centering
    \includegraphics[width=\textwidth]{Austria-DP.png}\\[1em] {Österreich}
  \end{minipage}
\end{center}


\section{Literatur}

\begin{itemize}
\item Hans-Jürgen Elschenbroich. Corona: Mathematik \& Modellbildung.\\
  \url{https://www.geogebra.org/m/cfammtpe}.  2020.
\item Joachim Engel. Parameterschätzen in logistischen Wachstumsmodellen.
  Stochastik in der Schule 30 (2010) 1, S. 13–18.
\item Hans-Gert Gräbe. Computeralgebra im Abitur. Reihe „Eagle Starthilfe“.
  Eagle Verlag, Leipzig 2018.  Siehe
  \url{https://hg-graebe.de/CAimAbitur/index.html}.
\item Maxima. \url{http://maxima.sourceforge.net/de/}. Das CAS ist in
  Linux-Distributionen über den Paketmanager leicht zu installieren.
\item Werner Rügemer. „Die USA haben das sicherste Gesundheitssystem der Welt“
  – Die Johns Hopkins University und das globale Pandemien-Management.
  01.04.2020.\\ \url{https://www.nachdenkseiten.de/?p=59825}
\end{itemize}

\section{Grafiken}

\begin{center}
  \includegraphics[width=.8\textwidth]{Germany.png}\\[1em]
  {Szenario für Deutschland (82.9 Mio. Einwohner)}
  
  \includegraphics[width=.8\textwidth]{Italy.png}\\[1em]
  {Szenario für Italien (60.4 Mio. Einwohner)}

  \includegraphics[width=.8\textwidth]{Spain.png}\\[1em]
  {Szenario für Spanien (46.7 Mio. Einwohner)}
    
  \includegraphics[width=.8\textwidth]{Sweden.png}\\[1em]
  {Szenario für Schweden (66.4 Mio. Einwohner)}
    
  \includegraphics[width=.8\textwidth]{Austria.png}\\[1em]
  {Szenario für Österreich (8.85 Mio. Einwohner)}
                  
  \includegraphics[width=.8\textwidth]{Hubei.png}\\[1em]
  {Szenario für China, Provinz Hubei (58.5 Mio. Einwohner)}
\end{center}


\end{document}
